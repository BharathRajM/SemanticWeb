{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader \n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import functools as ftools\n",
    "import itertools as it\n",
    "import collections\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# JSON formatting functions\n",
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    training_data = []\n",
    "    lines=[]\n",
    "    with open(dataturks_JSON_FilePath, 'r',encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        text = data['content'].replace(\"\\n\", \" \")\n",
    "        entities = []\n",
    "        data_annotations = data['annotation']\n",
    "        if data_annotations is not None:\n",
    "            for annotation in data_annotations:\n",
    "                #only a single point in text annotation.\n",
    "                point = annotation['points'][0]\n",
    "                labels = annotation['label']\n",
    "                # handle both list of labels or a single label.\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "\n",
    "                for label in labels:\n",
    "                    point_start = point['start']\n",
    "                    point_end = point['end']\n",
    "                    point_text = point['text']\n",
    "\n",
    "                    lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
    "                    rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
    "                    if lstrip_diff != 0:\n",
    "                        point_start = point_start + lstrip_diff\n",
    "                    if rstrip_diff != 0:\n",
    "                        point_end = point_end - rstrip_diff\n",
    "                    entities.append((point_start, point_end + 1 , label))\n",
    "        training_data.append((text, {\"entities\" : entities}))\n",
    "    return training_data\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Afreen Jamadar Active member of IIIT Committee in Third year  Sangli, Maharashtra - Email me on Indeed: indeed.com/r/Afreen-Jamadar/8baf379b705e37c6  I wish to use my knowledge, skills and conceptual understanding to create excellent team environments and work consistently achieving organization objectives believes in taking initiative and work to excellence in my work.  WORK EXPERIENCE  Active member of IIIT Committee in Third year  Cisco Networking -  Kanpur, Uttar Pradesh  organized by Techkriti IIT Kanpur and Azure Skynet. PERSONALLITY TRAITS: • Quick learning ability • hard working  EDUCATION  PG-DAC  CDAC ACTS  2017  Bachelor of Engg in Information Technology  Shivaji University Kolhapur -  Kolhapur, Maharashtra  2016  SKILLS  Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT ACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)  ADDITIONAL INFORMATION  TECHNICAL SKILLS:  • Programming Languages: C, C++, Java, .net, php. • Web Designing: HTML, XML • Operating Systems: Windows […] Windows Server 2003, Linux. • Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.  https://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN',\n",
       " {'entities': [[1155, 1199, 'Email Address'],\n",
       "   [743, 1141, 'Skills'],\n",
       "   [729, 733, 'Graduation Year'],\n",
       "   [675, 702, 'College Name'],\n",
       "   [631, 673, 'Degree'],\n",
       "   [625, 629, 'Graduation Year'],\n",
       "   [614, 623, 'College Name'],\n",
       "   [606, 612, 'Degree'],\n",
       "   [104, 148, 'Email Address'],\n",
       "   [62, 68, 'Location'],\n",
       "   [0, 14, 'Name']]}]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = trim_entity_spans(convert_dataturks_to_spacy(\"data/traindata.json\"))\n",
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>annotation</th>\n",
       "      <th>extras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhishek Jha Application Development Associate...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 12...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afreen Jamadar Active member of IIIT Committee...</td>\n",
       "      <td>[{'label': ['Email Address'], 'points': [{'sta...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akhil Yadav Polemaina Hyderabad, Telangana - E...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 37...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alok Khandai Operational Analyst (SQL DBA) Eng...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 80...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ananya Chavan lecturer - oracle tutorials  Mum...</td>\n",
       "      <td>[{'label': ['Degree'], 'points': [{'start': 20...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>Mansi Thanki Student  Jamnagar, Gujarat - Emai...</td>\n",
       "      <td>[{'label': ['College Name'], 'points': [{'star...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Anil Kumar Microsoft Azure (Basic Management) ...</td>\n",
       "      <td>[{'label': ['Location'], 'points': [{'start': ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Siddharth Choudhary Microsoft Office Suite - E...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 78...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Valarmathi Dhandapani Investment Banking Opera...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 92...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Pradeep Kumar Security Analyst in Infosys - Ca...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 58...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content  \\\n",
       "0    Abhishek Jha Application Development Associate...   \n",
       "1    Afreen Jamadar Active member of IIIT Committee...   \n",
       "2    Akhil Yadav Polemaina Hyderabad, Telangana - E...   \n",
       "3    Alok Khandai Operational Analyst (SQL DBA) Eng...   \n",
       "4    Ananya Chavan lecturer - oracle tutorials  Mum...   \n",
       "..                                                 ...   \n",
       "215  Mansi Thanki Student  Jamnagar, Gujarat - Emai...   \n",
       "216  Anil Kumar Microsoft Azure (Basic Management) ...   \n",
       "217  Siddharth Choudhary Microsoft Office Suite - E...   \n",
       "218  Valarmathi Dhandapani Investment Banking Opera...   \n",
       "219  Pradeep Kumar Security Analyst in Infosys - Ca...   \n",
       "\n",
       "                                            annotation  extras  \n",
       "0    [{'label': ['Skills'], 'points': [{'start': 12...     NaN  \n",
       "1    [{'label': ['Email Address'], 'points': [{'sta...     NaN  \n",
       "2    [{'label': ['Skills'], 'points': [{'start': 37...     NaN  \n",
       "3    [{'label': ['Skills'], 'points': [{'start': 80...     NaN  \n",
       "4    [{'label': ['Degree'], 'points': [{'start': 20...     NaN  \n",
       "..                                                 ...     ...  \n",
       "215  [{'label': ['College Name'], 'points': [{'star...     NaN  \n",
       "216  [{'label': ['Location'], 'points': [{'start': ...     NaN  \n",
       "217  [{'label': ['Skills'], 'points': [{'start': 78...     NaN  \n",
       "218  [{'label': ['Skills'], 'points': [{'start': 92...     NaN  \n",
       "219  [{'label': ['Skills'], 'points': [{'start': 58...     NaN  \n",
       "\n",
       "[220 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_json(\"data/traindata.json\", lines = True)\n",
    "df_data[\"content\"] = df_data[\"content\"].apply(lambda x: x.replace(\"\\n\", \" \"))\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/txetx/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_content</th>\n",
       "      <th>entities_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhishek Jha Application Development Associate...</td>\n",
       "      <td>[Name, Name, Designation, Designation, Designa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afreen Jamadar Active member IIIT Committee Th...</td>\n",
       "      <td>[Name, Name, O, O, O, O, O, O, O, O, Email Add...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akhil Yadav Polemaina Telangana Email indeed.c...</td>\n",
       "      <td>[Name, Name, Name, O, O, Email Address, Email ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alok Khandai Operational Analyst Engineer UNIS...</td>\n",
       "      <td>[Name, Name, Designation, Designation, Designa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ananya Chavan lecturer oracle tutorials Mahara...</td>\n",
       "      <td>[Name, Name, Designation, Companies worked at,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>Mansi Thanki Student Gujarat Email indeed.com/...</td>\n",
       "      <td>[Name, Name, Designation, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Anil Kumar Microsoft Azure Delhi Email indeed....</td>\n",
       "      <td>[Name, Name, Designation, Designation, Locatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Siddharth Choudhary Microsoft Office Suite Exp...</td>\n",
       "      <td>[Name, Name, Designation, Designation, Designa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Valarmathi Dhandapani Investment Banking Karna...</td>\n",
       "      <td>[Name, Name, Designation, Designation, O, O, E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Pradeep Kumar Security Analyst Infosys Career ...</td>\n",
       "      <td>[Name, Name, Designation, Designation, Compani...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_content  \\\n",
       "0    Abhishek Jha Application Development Associate...   \n",
       "1    Afreen Jamadar Active member IIIT Committee Th...   \n",
       "2    Akhil Yadav Polemaina Telangana Email indeed.c...   \n",
       "3    Alok Khandai Operational Analyst Engineer UNIS...   \n",
       "4    Ananya Chavan lecturer oracle tutorials Mahara...   \n",
       "..                                                 ...   \n",
       "215  Mansi Thanki Student Gujarat Email indeed.com/...   \n",
       "216  Anil Kumar Microsoft Azure Delhi Email indeed....   \n",
       "217  Siddharth Choudhary Microsoft Office Suite Exp...   \n",
       "218  Valarmathi Dhandapani Investment Banking Karna...   \n",
       "219  Pradeep Kumar Security Analyst Infosys Career ...   \n",
       "\n",
       "                                       entities_mapped  \n",
       "0    [Name, Name, Designation, Designation, Designa...  \n",
       "1    [Name, Name, O, O, O, O, O, O, O, O, Email Add...  \n",
       "2    [Name, Name, Name, O, O, Email Address, Email ...  \n",
       "3    [Name, Name, Designation, Designation, Designa...  \n",
       "4    [Name, Name, Designation, Companies worked at,...  \n",
       "..                                                 ...  \n",
       "215  [Name, Name, Designation, O, O, O, O, O, O, O,...  \n",
       "216  [Name, Name, Designation, Designation, Locatio...  \n",
       "217  [Name, Name, Designation, Designation, Designa...  \n",
       "218  [Name, Name, Designation, Designation, O, O, E...  \n",
       "219  [Name, Name, Designation, Designation, Compani...  \n",
       "\n",
       "[220 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "en_stops = set(stopwords.words('english'))\n",
    "\n",
    "df_data = pd.DataFrame(columns=['clean_content','entities_mapped'])\n",
    "entities_mapped = []\n",
    "clean_content = []\n",
    "for i in range(len(data)):\n",
    "    content=data[i][0].split()\n",
    "    entities=data[i][1]['entities']\n",
    "    words=[]\n",
    "    labels=[]\n",
    "    \n",
    "    for word in content:\n",
    "        \n",
    "        if ((word.isalnum() or word.find(\".com\")!=-1) and word not in en_stops):\n",
    "            words.append(word)\n",
    "            found = False\n",
    "            \n",
    "            for entity in sorted(entities):\n",
    "                ent_start = entity[0]\n",
    "                ent_end = entity[1]\n",
    "                ent_label = entity[2]\n",
    "                \n",
    "                if word in data[i][0][ent_start:ent_end].split():\n",
    "                    labels.append(ent_label)\n",
    "                    found = True\n",
    "                    break\n",
    "                    \n",
    "            if not found:\n",
    "                labels.append(\"O\")\n",
    "              \n",
    "    entities_mapped.append(labels)\n",
    "    clean_content.append(words)\n",
    "    \n",
    "df_data = pd.DataFrame(columns = [\"clean_content\", \"entities_mapped\"])\n",
    "df_data[\"entities_mapped\"] = entities_mapped\n",
    "df_data[\"clean_content\"] = clean_content\n",
    "df_data[\"clean_content\"] = df_data[\"clean_content\"].apply(lambda x: \" \".join(x))\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that words are aligned wit labels\n",
    "assert all((len(d1) == len(d2.split()) for d1,d2 in zip(df_data['entities_mapped'].iloc, df_data['clean_content'].iloc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "MIN_LEN = 0\n",
    "STRIDE = 32\n",
    "bs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_grams = df_data.clean_content.map(str.split).tolist()\n",
    "two_grams = [list(map(\" \".join,zip(og[:-1],og[1:]))) for og in one_grams]\n",
    "three_grams = [list(map(\" \".join,zip(og[:-2],og[1:-1],og[2:]))) for og in one_grams]\n",
    "\n",
    "df_data[\"one_grams\"] = one_grams\n",
    "df_data[\"two_grams\"] = two_grams\n",
    "df_data[\"three_grams\"] = three_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.to_csv(\"data/traindata_ngrams.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels(text,labels):\n",
    "    tokens = text.split()\n",
    "    labels_aligned = []\n",
    "    \n",
    "    for token,label in zip(tokens,labels):\n",
    "        sub_tokens = tokenizer(token)\n",
    "        labels_aligned += [label]*(len(sub_tokens[\"input_ids\"]) - 2)\n",
    "    \n",
    "    return labels_aligned\n",
    "\n",
    "def spanning_window(input_ids,\n",
    "                    attention_mask,\n",
    "                    token_type_ids,\n",
    "                    labels,\n",
    "                    w_size,\n",
    "                    stride,\n",
    "                    w_min):\n",
    "    \n",
    "    input_ids_post = []\n",
    "    attention_mask_post = []\n",
    "    token_type_ids_post = []\n",
    "    labels_post = []\n",
    "    \n",
    "    for a,b,c,d in zip(input_ids,attention_mask,token_type_ids,labels):\n",
    "        for begin_i in range(0,len(d),stride):\n",
    "            bi1 = begin_i\n",
    "            bi2 = begin_i + 1\n",
    "            \n",
    "            iid = [101] + a[bi2:bi2+w_size-2]\n",
    "            am = [1] + b[bi2:bi2+w_size-2]\n",
    "            tti = [0] + c[bi2:bi2+w_size-2]\n",
    "            lb = d[bi1:bi1+w_size-2]\n",
    "            \n",
    "            if len(lb) == (w_size - 2):\n",
    "                iid = iid + [102 if iid[-1] != 102 else 0]\n",
    "                am = am + [1 if iid[-1] == 102 else 0]\n",
    "                tti = tti + [0]\n",
    "                lb = lb + [\"O\"]\n",
    "            else:\n",
    "                if len(iid) < w_min:\n",
    "                    continue\n",
    "                \n",
    "                missing_length = w_size - 2 - len(lb)\n",
    "                iid = iid + [0]*missing_length\n",
    "                am = am + [0]*missing_length\n",
    "                tti = tti + [0]*missing_length\n",
    "                lb = lb + [\"O\"]*(missing_length+1)\n",
    "                \n",
    "            input_ids_post.append(iid)\n",
    "            attention_mask_post.append(am)\n",
    "            token_type_ids_post.append(tti)\n",
    "            labels_post.append(lb)\n",
    "            \n",
    "    return (input_ids_post,\n",
    "            attention_mask_post,\n",
    "            token_type_ids_post,\n",
    "            labels_post)\n",
    "\n",
    "tags_vals = [\"O\",\"Degree\",\"Designation\",\"Skills\",\"Name\",\"College Name\",\"Email Address\",\"Companies worked at\",\"Empty\",\"Graduation Year\",\"Years of Experience\",\"Location\",\"UNKNOWN\"]\n",
    "tag2idx = {t: i for i,t in enumerate(tags_vals)}\n",
    "#tag2idx = {t: 0 for i,t in enumerate(tags_vals)}\n",
    "\n",
    "#tag2idx[\"Degree\"] = 1\n",
    "#tag2idx[\"Designation\"] = 1\n",
    "#tag2idx[\"Skills\"] = 3\n",
    "\n",
    "\n",
    "def vectorize_df(df):\n",
    "    tokenized_texts = tokenizer(df[\"clean_content\"].tolist())\n",
    "    labels = [align_labels(txt,label) for txt,label in zip(df[\"clean_content\"],df['entities_mapped'])]\n",
    "\n",
    "    # Use spanning window\n",
    "    (tokenized_texts[\"input_ids\"],\n",
    "    tokenized_texts[\"attention_mask\"],\n",
    "    tokenized_texts[\"token_type_ids\"],\n",
    "    labels) = spanning_window(input_ids=tokenized_texts[\"input_ids\"],\n",
    "                                attention_mask=tokenized_texts[\"attention_mask\"],\n",
    "                                token_type_ids=tokenized_texts[\"token_type_ids\"],\n",
    "                                labels=labels,\n",
    "                                w_size=MAX_LEN,\n",
    "                                stride=STRIDE,\n",
    "                                w_min=MIN_LEN)\n",
    "    \n",
    "    label_ids = [list(map(tag2idx.__getitem__,lab)) for lab in labels]\n",
    "\n",
    "    tokenized_texts_pt = {k:torch.tensor(v) for k,v in tokenized_texts.items()}\n",
    "    label_ids_pt = torch.tensor(label_ids,dtype=torch.int64)\n",
    "    \n",
    "    return tokenized_texts_pt, label_ids_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 102, 0, 101, 103]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_data,test_size=0.1,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = vectorize_df(df_train)\n",
    "X_test, y_test = vectorize_df(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3022, 128])\n",
      "torch.Size([237, 128])\n"
     ]
    }
   ],
   "source": [
    "print(X_train[\"input_ids\"].size())\n",
    "print(X_test[\"input_ids\"].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "class ResumeNERLM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 out_classes=len(tags_vals),\n",
    "                 lm=\"dslim/bert-base-NER\",\n",
    "                 device=device):\n",
    "        super(ResumeNERLM,self).__init__()\n",
    "\n",
    "        conf = AutoConfig.from_pretrained(lm)\n",
    "        conf.output_hidden_states = True\n",
    "        model = AutoModelForTokenClassification.from_config(conf).to(device)\n",
    "        \n",
    "        classification_layer = nn.Linear(768,out_classes).to(device)\n",
    "        \n",
    "        self.model = model\n",
    "        self.classification_layer = classification_layer\n",
    "        \n",
    "        self.optimizer = optim.Adam(model.parameters(),lr=5e-5)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self,**x):\n",
    "        x = self.model(**x)\n",
    "        x = x[\"hidden_states\"][-1]\n",
    "        x = [self.classification_layer(x[:,i]) for i in range(1,x.size()[1])]\n",
    "        x = torch.stack(x,dim=1)\n",
    "                        \n",
    "        return x\n",
    "    \n",
    "    def fit(self,X,y,bs=bs,epochs=6):\n",
    "        model.train()\n",
    "        samples,seq_len = X[\"input_ids\"].size()\n",
    "        for e in range(epochs):\n",
    "            perm = np.random.permutation(samples)\n",
    "            \n",
    "            trange = tqdm.trange(0,samples,bs)\n",
    "            loss_arr = []\n",
    "            for b_start in trange:\n",
    "                self.optimizer.zero_grad()\n",
    "                b_slice = slice(b_start,b_start+bs)\n",
    "\n",
    "                xi = {k: v[perm[b_slice]].to(self.device) for k,v in X.items()}\n",
    "                yi = y[perm[b_slice]].to(self.device)\n",
    "\n",
    "                x = self(**xi)\n",
    "\n",
    "                am = xi[\"attention_mask\"].bool()\n",
    "                losses = torch.mean(torch.stack([self.loss(x[i,am[i,1:]],yi[i,am[i,1:]]) for i in range(len(am))]))\n",
    "                \n",
    "                loss_arr.append(losses.item())\n",
    "                trange.set_postfix(loss=np.mean(loss_arr))\n",
    "                                \n",
    "                losses.backward()\n",
    "                self.optimizer.step()\n",
    "        model.eval()\n",
    "                \n",
    "    def predict(self,X,bs=32,return_proba=False):\n",
    "        model.eval()\n",
    "        samples,seq_len = X[\"input_ids\"].size()\n",
    "        trange = tqdm.trange(0,samples,bs)\n",
    "        \n",
    "        outputs = []\n",
    "        with torch.no_grad():\n",
    "            for b_start in trange:\n",
    "                b_slice = slice(b_start,b_start+bs)\n",
    "\n",
    "                xi = {k: v[b_slice].to(self.device) for k,v in X.items()}\n",
    "                x = self(**xi)\n",
    "\n",
    "                outputs.append(x.cpu().numpy())\n",
    "\n",
    "        outputs = np.concatenate(outputs,axis=0)\n",
    "        if return_proba:\n",
    "            return outputs\n",
    "        return np.argmax(outputs,axis=2)\n",
    "        \n",
    "model = ResumeNERLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 378/378 [00:51<00:00,  7.30it/s, loss=0.708]\n",
      "100%|█████████████████████████████| 378/378 [00:52<00:00,  7.20it/s, loss=0.412]\n",
      "100%|██████████████████████████████| 378/378 [00:52<00:00,  7.22it/s, loss=0.33]\n",
      "100%|█████████████████████████████| 378/378 [00:52<00:00,  7.22it/s, loss=0.281]\n",
      "100%|█████████████████████████████| 378/378 [00:52<00:00,  7.20it/s, loss=0.239]\n",
      "100%|███████████████████████████████| 378/378 [00:52<00:00,  7.19it/s, loss=0.2]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train,epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train,epochs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00,  9.20it/s]\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_f = res[:,:32]\n",
    "y_test_f = y_test[:,:32]\n",
    "\n",
    "res_f = np.reshape(res_f,(-1,))\n",
    "y_test_f = np.reshape(y_test_f,(-1,))\n",
    "\n",
    "res_f_l = list(map(tags_vals.__getitem__,res_f))\n",
    "y_test_f_l = list(map(tags_vals.__getitem__,y_test_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "       College Name       0.58      0.39      0.46       186\n",
      "Companies worked at       0.63      0.68      0.65       216\n",
      "             Degree       0.57      0.46      0.51        93\n",
      "        Designation       0.60      0.37      0.46       198\n",
      "      Email Address       0.85      0.90      0.88       409\n",
      "    Graduation Year       0.31      0.38      0.34        24\n",
      "           Location       0.50      0.55      0.52        11\n",
      "               Name       0.84      0.96      0.89       118\n",
      "                  O       0.88      0.92      0.90      5788\n",
      "             Skills       0.44      0.31      0.36       528\n",
      "Years of Experience       0.30      0.23      0.26        13\n",
      "\n",
      "           accuracy                           0.83      7584\n",
      "          macro avg       0.59      0.56      0.57      7584\n",
      "       weighted avg       0.82      0.83      0.83      7584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_f_l,res_f_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting_pred(pred):\n",
    "    l_pred = []\n",
    "    for i in range(pred.shape[-1]):\n",
    "        pred_vote = []\n",
    "        for j,p in enumerate(pred):\n",
    "            if j*STRIDE <= i < j*STRIDE+MAX_LEN:\n",
    "                pred_vote.append(p[i-j*STRIDE])\n",
    "\n",
    "        c = collections.Counter(pred_vote).most_common()\n",
    "        l_pred.append(c[0][0])\n",
    "        \n",
    "    return l_pred\n",
    "\n",
    "def projection_pred(pred):\n",
    "    l_pred = []\n",
    "    for p in pred: # This can be done better\n",
    "        l_pred += list(p[:STRIDE])\n",
    "        \n",
    "    return l_pred\n",
    "\n",
    "def predict_entities(text,strict_merge=True):\n",
    "    \n",
    "    if not text:\n",
    "        return {}\n",
    "    \n",
    "    placeholder_labels = [\"O\"]*len(text.split())\n",
    "    placeholder_labels = align_labels(text,placeholder_labels)\n",
    "    tokenized_text = tokenizer([text])\n",
    "\n",
    "    (tokenized_text[\"input_ids\"],\n",
    "    tokenized_text[\"attention_mask\"],\n",
    "    tokenized_text[\"token_type_ids\"],\n",
    "    _) = spanning_window(input_ids=tokenized_text[\"input_ids\"],\n",
    "                                attention_mask=tokenized_text[\"attention_mask\"],\n",
    "                                token_type_ids=tokenized_text[\"token_type_ids\"],\n",
    "                                labels=[placeholder_labels],\n",
    "                                w_size=MAX_LEN,\n",
    "                                stride=STRIDE,\n",
    "                                w_min=MIN_LEN)\n",
    "    \n",
    "    tokenized_text_pt = {k:torch.tensor(v) for k,v in tokenized_text.items()}\n",
    "    \n",
    "    pred = model.predict(tokenized_text_pt)\n",
    "    \n",
    "    l_pred = []\n",
    "    tt = []\n",
    "    \n",
    "    for iid in tokenized_text[\"input_ids\"]:\n",
    "        tt += list(iid[1:STRIDE+1])\n",
    "    \n",
    "    if strict_merge:\n",
    "        l_pred = voting_pred(pred)    \n",
    "    else:\n",
    "        l_pred = projection_pred(pred)\n",
    "    \n",
    "        \n",
    "    l_pred = list(map(tags_vals.__getitem__,l_pred))\n",
    "    \n",
    "    spans = []\n",
    "    for i,p in enumerate(l_pred):\n",
    "        if p == \"O\":\n",
    "            continue\n",
    "        if not spans or spans[-1][0] != p:\n",
    "            spans.append([p,i,i+1])\n",
    "        else:\n",
    "            spans[-1] = spans[-1][:2] + [i+1]\n",
    "        \n",
    "    d = collections.defaultdict(list)\n",
    "    for s in spans:\n",
    "        dec = tokenizer.decode([101] + tt[s[1]:s[2]] + [102])\n",
    "        dec = dec.replace(\"[CLS]\",\"\").replace(\"[SEP]\",\"\")\n",
    "        d[s[0]].append(dec)\n",
    "        \n",
    "    return dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_entities(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Santosh Ganta Senior Systems Engineer - mainframe  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Santosh-Ganta/4270d63f03e71ee8  Willing to relocate to: Bengaluru, Karnataka - hyderbad, Telangana - Chennai, Tamil Nadu  WORK EXPERIENCE  Senior Systems Engineer  Infosys Limited -  Chennai, Tamil Nadu -  February 2014 to Present  Development,Testing,Support  Senior system engineer  Infosys limited  Development,Testing,Support  EDUCATION  B.Tech in Information Technology  GMR Institute of Technology and Management -  Kakinada, Andhra Pradesh  2013  Pratibha Junior College  2009  English, Hindi  S.R high School -  Chennai, Tamil Nadu  2006  SKILLS  CA7 (4 years), DB2 (4 years), QMF (4 years), Cobol (4 years), Mainframe (4 years), Cics (4 years), Rexx (4 years)  ADDITIONAL INFORMATION  • Adopt to any kind of Environment.  Technical Summary  https://www.indeed.com/r/Santosh-Ganta/4270d63f03e71ee8?isid=rex-download&ikw=download-top&co=IN   • Tools: ISPF, SPUFI, QMF, File-Aid, MainView, Librarian, CA7, Control-M, Xpeditor • Operating System: Windows 7 • Database: DB2, SQL Server • Domain: Retail • Packages: MS office  • Secondary Skills: Java Script, HTML, JSP, Java, Oracle 10g, Unix',\n",
       " {'entities': [[958, 1203, 'Skills'],\n",
       "   [662, 775, 'Skills'],\n",
       "   [648, 652, 'Graduation Year'],\n",
       "   [608, 646, 'College Name'],\n",
       "   [586, 590, 'Graduation Year'],\n",
       "   [561, 584, 'College Name'],\n",
       "   [555, 559, 'Graduation Year'],\n",
       "   [483, 525, 'College Name'],\n",
       "   [449, 481, 'Degree'],\n",
       "   [271, 286, 'Companies worked at'],\n",
       "   [246, 269, 'Designation'],\n",
       "   [163, 172, 'Location'],\n",
       "   [94, 137, 'Email Address'],\n",
       "   [51, 60, 'Location'],\n",
       "   [14, 37, 'Designation'],\n",
       "   [0, 13, 'Name']]}]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[df_test.index[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract entities\n",
    "## Resumes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Resume Title</th>\n",
       "      <th>Introduction</th>\n",
       "      <th>Work Experience</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Additional Information</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sales Manager</td>\n",
       "      <td>Dynamic technical sales professional with dive...</td>\n",
       "      <td>Sales Manager-MadgeTech, Inc-August 2015 to Fe...</td>\n",
       "      <td>120 months-CRM,72 months-Contract Negotiation,...</td>\n",
       "      <td>    Well-Developed Sales &amp; Business Acumen   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Implementation Engineer</td>\n",
       "      <td>Experienced, dependable and motivated IT Techn...</td>\n",
       "      <td>Implementation Engineer-Versatile Communicatio...</td>\n",
       "      <td>15 months-CISCO,12 months-FIBER OPTIC,6 months...</td>\n",
       "      <td>TECHNICAL SKILLS\\n\\nHardware: Switches, Router...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Civil engineer</td>\n",
       "      <td>To obtain full time employment in the field of...</td>\n",
       "      <td>Engineering Department Intern-Town of Billeric...</td>\n",
       "      <td>NaN</td>\n",
       "      <td> Bachelors of Science in Civil and Environmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BDC Data Analyst</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BDC Data Analyst-Gary Rome Auto Group-January ...</td>\n",
       "      <td>30 months-SIX-SIGMA,36 months-DATA ANALYSIS,24...</td>\n",
       "      <td>CORE COMPETENCIES\\n Project Management Team ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Safety Engineer Intern</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Safety Engineer Intern-Hexagon Manufacturing I...</td>\n",
       "      <td>9 months-MATLAB,36 months-OPTIMIZATION,36 mont...</td>\n",
       "      <td>Core Competencies: Control Systems, Automotive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Classified Ads Manager</td>\n",
       "      <td>To utilize experience and personal skills in t...</td>\n",
       "      <td>Classified Ads Manager-Quality of Life Publica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ASSISTANT PROGRAM MANAGER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASSISTANT PROGRAM MANAGER-HARBOR HOMES-August ...</td>\n",
       "      <td>13 months-PROGRAM MANAGER,0 months-RETAIL,13 m...</td>\n",
       "      <td>Skills &amp; Abilities\\nMANAGEMENT\\n 4 years of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Technical Customer Service</td>\n",
       "      <td>High energy, hardworking Engineering graduate ...</td>\n",
       "      <td>Technical Customer Service-SmartCo Services LL...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data scientist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Skills * Programming Languages: Python (pandas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>programmer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOFTWARE SKILLS: â?¢ General Computer Proficie...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lawyer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Skills Legal Writing Efficient researcher Lega...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>event manager</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Additional qualifications April 2000, Web Desi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Web developer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Technical Skills Web Technologies: Angular JS,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mechanical engineer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Education Details \\nJune 2014 to June 2018 BE ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Seller</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KEY SKILLS: â?¢ Planning &amp; Strategizing â?¢ Pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Personal trainer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Education Details \\nJanuary 2009 P.G. Sports s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Resume Title  \\\n",
       "0                Sales Manager   \n",
       "1      Implementation Engineer   \n",
       "2               Civil engineer   \n",
       "3             BDC Data Analyst   \n",
       "4       Safety Engineer Intern   \n",
       "5       Classified Ads Manager   \n",
       "6    ASSISTANT PROGRAM MANAGER   \n",
       "7   Technical Customer Service   \n",
       "8               Data scientist   \n",
       "9                   programmer   \n",
       "10                      lawyer   \n",
       "11               event manager   \n",
       "12               Web developer   \n",
       "13         Mechanical engineer   \n",
       "14                      Seller   \n",
       "15            Personal trainer   \n",
       "\n",
       "                                         Introduction  \\\n",
       "0   Dynamic technical sales professional with dive...   \n",
       "1   Experienced, dependable and motivated IT Techn...   \n",
       "2   To obtain full time employment in the field of...   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5   To utilize experience and personal skills in t...   \n",
       "6                                                 NaN   \n",
       "7   High energy, hardworking Engineering graduate ...   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "15                                                NaN   \n",
       "\n",
       "                                      Work Experience  \\\n",
       "0   Sales Manager-MadgeTech, Inc-August 2015 to Fe...   \n",
       "1   Implementation Engineer-Versatile Communicatio...   \n",
       "2   Engineering Department Intern-Town of Billeric...   \n",
       "3   BDC Data Analyst-Gary Rome Auto Group-January ...   \n",
       "4   Safety Engineer Intern-Hexagon Manufacturing I...   \n",
       "5   Classified Ads Manager-Quality of Life Publica...   \n",
       "6   ASSISTANT PROGRAM MANAGER-HARBOR HOMES-August ...   \n",
       "7   Technical Customer Service-SmartCo Services LL...   \n",
       "8   Skills * Programming Languages: Python (pandas...   \n",
       "9   SOFTWARE SKILLS: â?¢ General Computer Proficie...   \n",
       "10  Skills Legal Writing Efficient researcher Lega...   \n",
       "11  Additional qualifications April 2000, Web Desi...   \n",
       "12  Technical Skills Web Technologies: Angular JS,...   \n",
       "13  Education Details \\nJune 2014 to June 2018 BE ...   \n",
       "14  KEY SKILLS: â?¢ Planning & Strategizing â?¢ Pr...   \n",
       "15  Education Details \\nJanuary 2009 P.G. Sports s...   \n",
       "\n",
       "                                               Skills  \\\n",
       "0   120 months-CRM,72 months-Contract Negotiation,...   \n",
       "1   15 months-CISCO,12 months-FIBER OPTIC,6 months...   \n",
       "2                                                 NaN   \n",
       "3   30 months-SIX-SIGMA,36 months-DATA ANALYSIS,24...   \n",
       "4   9 months-MATLAB,36 months-OPTIMIZATION,36 mont...   \n",
       "5                                                 NaN   \n",
       "6   13 months-PROGRAM MANAGER,0 months-RETAIL,13 m...   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "15                                                NaN   \n",
       "\n",
       "                               Additional Information  \n",
       "0       Well-Developed Sales & Business Acumen   ...  \n",
       "1   TECHNICAL SKILLS\\n\\nHardware: Switches, Router...  \n",
       "2    Bachelors of Science in Civil and Environmen...  \n",
       "3   CORE COMPETENCIES\\n Project Management Team ...  \n",
       "4   Core Competencies: Control Systems, Automotive...  \n",
       "5                                                 NaN  \n",
       "6   Skills & Abilities\\nMANAGEMENT\\n 4 years of m...  \n",
       "7                                                 NaN  \n",
       "8                                                 NaN  \n",
       "9                                                 NaN  \n",
       "10                                                NaN  \n",
       "11                                                NaN  \n",
       "12                                                NaN  \n",
       "13                                                NaN  \n",
       "14                                                NaN  \n",
       "15                                                NaN  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumes = pd.read_csv(\"data/resumes_indeed_com-job_sample_1.csv\",encoding=\"ISO-8859-15\")\n",
    "\n",
    "resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 31.11it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 75.13it/s]\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 109.80it/s]\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 108.94it/s]\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 109.11it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 10.25it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 19.97it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 32.27it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 12.79it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 20.72it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 22.82it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 12.54it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.54it/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 16.29it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 34.03it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 29.04it/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 11.79it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 11.18it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 17.24it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 11.23it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 20.45it/s]\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 122.04it/s]\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 122.54it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 52.42it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 84.92it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 85.26it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 84.90it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 52.88it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 86.12it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 87.27it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 87.58it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 42.62it/s]\n"
     ]
    }
   ],
   "source": [
    "resume_cols = [\"Introduction\",\"Work Experience\",\"Skills\",\"Additional Information\"]\n",
    "\n",
    "ld = [collections.defaultdict(list) for _ in resumes.index]\n",
    "for rc in resume_cols: \n",
    "    resumes[rc][resumes[rc].isnull()] = \"\" # Fill nan\n",
    "    \n",
    "    for d,r in zip(ld,resumes[rc]):\n",
    "        l = predict_entities(r,strict_merge=False)\n",
    "\n",
    "        for k,v in l.items():\n",
    "            d[k].extend(v)\n",
    "            \n",
    "ld = list(map(dict,ld))\n",
    "ld = {i:v for i,v in enumerate(ld)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"resumes_ner_soft.json\",\"w\") as f:\n",
    "    json.dump(ld,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 14.98it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 10.29it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 26.94it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 19.63it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.57it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 34.48it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 28.58it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 23.01it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 41.10it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 23.01it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 21.06it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 17.44it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 11.09it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 11.53it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.52it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 12.75it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.10it/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 12.63it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.95it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.72it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.45it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 29.23it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.97it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 19.38it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 17.32it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 34.24it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 22.01it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 17.32it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.16it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 11.14it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 40.92it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 23.27it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 22.46it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 29.73it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 17.22it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 20.62it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.68it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 29.74it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 34.47it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 22.89it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 28.56it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 17.34it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 12.55it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 37.23it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 20.56it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 17.16it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 22.92it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 12.73it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 29.26it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.92it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 34.29it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.59it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 21.40it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 33.98it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 22.58it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 17.25it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.79it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 12.69it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 11.79it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.65it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 11.62it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 20.45it/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 15.17it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.80it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 11.12it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.37it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.29it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 10.29it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.18it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.83it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 21.04it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 20.77it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 34.77it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 19.66it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.12it/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 12.72it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 37.63it/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 14.15it/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 13.40it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 10.92it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.90it/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 16.02it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.57it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 27.99it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.28it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 28.40it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 82.52it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.48it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 17.19it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 17.22it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 19.16it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 11.69it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 12.65it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 21.40it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 22.60it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 22.57it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 12.71it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 21.89it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 23.14it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 17.29it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.60it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 29.23it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 41.10it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 22.12it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 19.52it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 28.62it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 20.93it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 17.28it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 20.14it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 20.78it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.61it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 20.10it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.47it/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 12.71it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.65it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 34.04it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 28.59it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 34.24it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 10.99it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 10.40it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 15.75it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 10.98it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.85it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 11.05it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.49it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 14.11it/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 10.04it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.84it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.20it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 28.63it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 50.97it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.28it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 21.49it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 39.75it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 28.00it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.20it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 21.16it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 42.23it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 28.95it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 20.26it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 22.97it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 17.37it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 28.42it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 62.70it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.63it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.97it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.99it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 20.42it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 52.68it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.11it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 29.08it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.85it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  9.24it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 36.60it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 17.23it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 21.17it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 15.81it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 21.48it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 28.02it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 33.94it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.70it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 34.42it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 21.89it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 11.78it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 27.44it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 28.81it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 22.23it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 19.34it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 22.11it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.74it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.96it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 29.25it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 29.20it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 19.29it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 28.56it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.55it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 28.44it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 28.95it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 28.11it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 62.68it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 41.03it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 12.50it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 28.78it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.60it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 34.43it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 21.84it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 41.45it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.22it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 51.22it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 33.62it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.45it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 20.43it/s]\n"
     ]
    }
   ],
   "source": [
    "job_proposals = pd.read_csv(\"data/job_proposals_modified.csv\",encoding=\"ISO-8859-15\")\n",
    "\n",
    "jp = {i:predict_entities(v,strict_merge=False) for i,v in enumerate(job_proposals.job_description)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"job_proposals_ner_soft.json\",\"w\") as f:\n",
    "    json.dump(jp,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
