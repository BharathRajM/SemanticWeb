{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader \n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import functools as ftools\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# JSON formatting functions\n",
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    training_data = []\n",
    "    lines=[]\n",
    "    with open(dataturks_JSON_FilePath, 'r',encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        text = data['content'].replace(\"\\n\", \" \")\n",
    "        entities = []\n",
    "        data_annotations = data['annotation']\n",
    "        if data_annotations is not None:\n",
    "            for annotation in data_annotations:\n",
    "                #only a single point in text annotation.\n",
    "                point = annotation['points'][0]\n",
    "                labels = annotation['label']\n",
    "                # handle both list of labels or a single label.\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "\n",
    "                for label in labels:\n",
    "                    point_start = point['start']\n",
    "                    point_end = point['end']\n",
    "                    point_text = point['text']\n",
    "\n",
    "                    lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
    "                    rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
    "                    if lstrip_diff != 0:\n",
    "                        point_start = point_start + lstrip_diff\n",
    "                    if rstrip_diff != 0:\n",
    "                        point_end = point_end - rstrip_diff\n",
    "                    entities.append((point_start, point_end + 1 , label))\n",
    "        training_data.append((text, {\"entities\" : entities}))\n",
    "    return training_data\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Abhishek Jha Application Development Associate - Accenture  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a  • To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and company's growth in best possible ways.  Willing to relocate to: Bangalore, Karnataka  WORK EXPERIENCE  Application Development Associate  Accenture -  November 2017 to Present  Role: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries for the Bot which will be triggered based on given input. Also, Training the bot for different possible utterances (Both positive and negative), which will be given as input by the user.  EDUCATION  B.E in Information science and engineering  B.v.b college of engineering and technology -  Hubli, Karnataka  August 2013 to June 2017  12th in Mathematics  Woodbine modern school  April 2011 to March 2013  10th  Kendriya Vidyalaya  April 2001 to March 2011  SKILLS  C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year), Database Management System (Less than 1 year), Java (Less than 1 year)  ADDITIONAL INFORMATION  Technical Skills  https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN   • Programming language: C, C++, Java • Oracle PeopleSoft • Internet Of Things • Machine Learning • Database Management System • Computer Networks • Operating System worked on: Linux, Windows, Mac  Non - Technical Skills  • Honest and Hard-Working • Tolerant and Flexible to Different Situations • Polite and Calm • Team-Player\",\n",
       " {'entities': [[1296, 1622, 'Skills'],\n",
       "   [993, 1154, 'Skills'],\n",
       "   [939, 957, 'College Name'],\n",
       "   [883, 905, 'College Name'],\n",
       "   [856, 860, 'Graduation Year'],\n",
       "   [771, 814, 'College Name'],\n",
       "   [727, 769, 'Designation'],\n",
       "   [407, 416, 'Companies worked at'],\n",
       "   [372, 405, 'Designation'],\n",
       "   [95, 145, 'Email Address'],\n",
       "   [60, 69, 'Location'],\n",
       "   [49, 58, 'Companies worked at'],\n",
       "   [13, 46, 'Designation'],\n",
       "   [0, 12, 'Name']]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = trim_entity_spans(convert_dataturks_to_spacy(\"data/traindata.json\"))\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>annotation</th>\n",
       "      <th>extras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhishek Jha Application Development Associate...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 12...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afreen Jamadar Active member of IIIT Committee...</td>\n",
       "      <td>[{'label': ['Email Address'], 'points': [{'sta...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akhil Yadav Polemaina Hyderabad, Telangana - E...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 37...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alok Khandai Operational Analyst (SQL DBA) Eng...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 80...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ananya Chavan lecturer - oracle tutorials  Mum...</td>\n",
       "      <td>[{'label': ['Degree'], 'points': [{'start': 20...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>Mansi Thanki Student  Jamnagar, Gujarat - Emai...</td>\n",
       "      <td>[{'label': ['College Name'], 'points': [{'star...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Anil Kumar Microsoft Azure (Basic Management) ...</td>\n",
       "      <td>[{'label': ['Location'], 'points': [{'start': ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Siddharth Choudhary Microsoft Office Suite - E...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 78...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Valarmathi Dhandapani Investment Banking Opera...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 92...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Pradeep Kumar Security Analyst in Infosys - Ca...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 58...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content  \\\n",
       "0    Abhishek Jha Application Development Associate...   \n",
       "1    Afreen Jamadar Active member of IIIT Committee...   \n",
       "2    Akhil Yadav Polemaina Hyderabad, Telangana - E...   \n",
       "3    Alok Khandai Operational Analyst (SQL DBA) Eng...   \n",
       "4    Ananya Chavan lecturer - oracle tutorials  Mum...   \n",
       "..                                                 ...   \n",
       "215  Mansi Thanki Student  Jamnagar, Gujarat - Emai...   \n",
       "216  Anil Kumar Microsoft Azure (Basic Management) ...   \n",
       "217  Siddharth Choudhary Microsoft Office Suite - E...   \n",
       "218  Valarmathi Dhandapani Investment Banking Opera...   \n",
       "219  Pradeep Kumar Security Analyst in Infosys - Ca...   \n",
       "\n",
       "                                            annotation  extras  \n",
       "0    [{'label': ['Skills'], 'points': [{'start': 12...     NaN  \n",
       "1    [{'label': ['Email Address'], 'points': [{'sta...     NaN  \n",
       "2    [{'label': ['Skills'], 'points': [{'start': 37...     NaN  \n",
       "3    [{'label': ['Skills'], 'points': [{'start': 80...     NaN  \n",
       "4    [{'label': ['Degree'], 'points': [{'start': 20...     NaN  \n",
       "..                                                 ...     ...  \n",
       "215  [{'label': ['College Name'], 'points': [{'star...     NaN  \n",
       "216  [{'label': ['Location'], 'points': [{'start': ...     NaN  \n",
       "217  [{'label': ['Skills'], 'points': [{'start': 78...     NaN  \n",
       "218  [{'label': ['Skills'], 'points': [{'start': 92...     NaN  \n",
       "219  [{'label': ['Skills'], 'points': [{'start': 58...     NaN  \n",
       "\n",
       "[220 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_json(\"data/traindata.json\", lines = True)\n",
    "df_data[\"content\"] = df_data[\"content\"].apply(lambda x: x.replace(\"\\n\", \" \"))\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/txetx/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_content</th>\n",
       "      <th>entities_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhishek Jha Application Development Associate...</td>\n",
       "      <td>[Name, Name, Designation, Designation, Designa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afreen Jamadar Active member IIIT Committee Th...</td>\n",
       "      <td>[Name, Name, O, O, O, O, O, O, O, O, Email Add...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akhil Yadav Polemaina Telangana Email indeed.c...</td>\n",
       "      <td>[Name, Name, Name, O, O, Email Address, Email ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alok Khandai Operational Analyst Engineer UNIS...</td>\n",
       "      <td>[Name, Name, Designation, Designation, Designa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ananya Chavan lecturer oracle tutorials Mahara...</td>\n",
       "      <td>[Name, Name, Designation, Companies worked at,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>Mansi Thanki Student Gujarat Email indeed.com/...</td>\n",
       "      <td>[Name, Name, Designation, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Anil Kumar Microsoft Azure Delhi Email indeed....</td>\n",
       "      <td>[Name, Name, Designation, Designation, Locatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Siddharth Choudhary Microsoft Office Suite Exp...</td>\n",
       "      <td>[Name, Name, Designation, Designation, Designa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Valarmathi Dhandapani Investment Banking Karna...</td>\n",
       "      <td>[Name, Name, Designation, Designation, O, O, E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Pradeep Kumar Security Analyst Infosys Career ...</td>\n",
       "      <td>[Name, Name, Designation, Designation, Compani...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_content  \\\n",
       "0    Abhishek Jha Application Development Associate...   \n",
       "1    Afreen Jamadar Active member IIIT Committee Th...   \n",
       "2    Akhil Yadav Polemaina Telangana Email indeed.c...   \n",
       "3    Alok Khandai Operational Analyst Engineer UNIS...   \n",
       "4    Ananya Chavan lecturer oracle tutorials Mahara...   \n",
       "..                                                 ...   \n",
       "215  Mansi Thanki Student Gujarat Email indeed.com/...   \n",
       "216  Anil Kumar Microsoft Azure Delhi Email indeed....   \n",
       "217  Siddharth Choudhary Microsoft Office Suite Exp...   \n",
       "218  Valarmathi Dhandapani Investment Banking Karna...   \n",
       "219  Pradeep Kumar Security Analyst Infosys Career ...   \n",
       "\n",
       "                                       entities_mapped  \n",
       "0    [Name, Name, Designation, Designation, Designa...  \n",
       "1    [Name, Name, O, O, O, O, O, O, O, O, Email Add...  \n",
       "2    [Name, Name, Name, O, O, Email Address, Email ...  \n",
       "3    [Name, Name, Designation, Designation, Designa...  \n",
       "4    [Name, Name, Designation, Companies worked at,...  \n",
       "..                                                 ...  \n",
       "215  [Name, Name, Designation, O, O, O, O, O, O, O,...  \n",
       "216  [Name, Name, Designation, Designation, Locatio...  \n",
       "217  [Name, Name, Designation, Designation, Designa...  \n",
       "218  [Name, Name, Designation, Designation, O, O, E...  \n",
       "219  [Name, Name, Designation, Designation, Compani...  \n",
       "\n",
       "[220 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "en_stops = set(stopwords.words('english'))\n",
    "\n",
    "df_data = pd.DataFrame(columns=['clean_content','entities_mapped'])\n",
    "entities_mapped = []\n",
    "clean_content = []\n",
    "for i in range(len(data)):\n",
    "    content=data[i][0].split()\n",
    "    entities=data[i][1]['entities']\n",
    "    words=[]\n",
    "    labels=[]\n",
    "    \n",
    "    for word in content:\n",
    "        \n",
    "        if ((word.isalnum() or word.find(\".com\")!=-1) and word not in en_stops):\n",
    "            words.append(word)\n",
    "            found = False\n",
    "            \n",
    "            for entity in sorted(entities):\n",
    "                ent_start = entity[0]\n",
    "                ent_end = entity[1]\n",
    "                ent_label = entity[2]\n",
    "                \n",
    "                if word in data[i][0][ent_start:ent_end].split():\n",
    "                    labels.append(ent_label)\n",
    "                    found = True\n",
    "                    break\n",
    "                    \n",
    "            if not found:\n",
    "                labels.append(\"O\")\n",
    "              \n",
    "    entities_mapped.append(labels)\n",
    "    clean_content.append(words)\n",
    "    \n",
    "df_data = pd.DataFrame(columns = [\"clean_content\", \"entities_mapped\"])\n",
    "df_data[\"entities_mapped\"] = entities_mapped\n",
    "df_data[\"clean_content\"] = clean_content\n",
    "df_data[\"clean_content\"] = df_data[\"clean_content\"].apply(lambda x: \" \".join(x))\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that words are aligned wit labels\n",
    "assert all((len(d1) == len(d2.split()) for d1,d2 in zip(df_data['entities_mapped'].iloc, df_data['clean_content'].iloc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "MIN_LEN = 32\n",
    "STRIDE = 32\n",
    "bs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels(text,labels):\n",
    "    tokens = text.split()\n",
    "    labels_aligned = []\n",
    "    \n",
    "    for token,label in zip(tokens,labels):\n",
    "        sub_tokens = tokenizer(token)\n",
    "        labels_aligned += [label]*(len(sub_tokens[\"input_ids\"]) - 2)\n",
    "    \n",
    "    return labels_aligned\n",
    "\n",
    "def spanning_window(input_ids,\n",
    "                    attention_mask,\n",
    "                    token_type_ids,\n",
    "                    labels,\n",
    "                    w_size,\n",
    "                    stride,\n",
    "                    w_min):\n",
    "    \n",
    "    input_ids_post = []\n",
    "    attention_mask_post = []\n",
    "    token_type_ids_post = []\n",
    "    labels_post = []\n",
    "    \n",
    "    for a,b,c,d in zip(input_ids,attention_mask,token_type_ids,labels):\n",
    "        for begin_i in range(0,len(d),stride):\n",
    "            bi1 = begin_i\n",
    "            bi2 = begin_i + 1\n",
    "            \n",
    "            iid = [101] + a[bi2:bi2+w_size-2]\n",
    "            am = [1] + b[bi2:bi2+w_size-2]\n",
    "            tti = [0] + c[bi2:bi2+w_size-2]\n",
    "            lb = d[bi1:bi1+w_size-2]\n",
    "            \n",
    "            if len(lb) == (w_size - 2):\n",
    "                iid = iid + [102 if iid[-1] != 102 else 0]\n",
    "                am = am + [1 if iid[-1] != 102 else 0]\n",
    "                tti = tti + [0]\n",
    "                lb = lb + [\"O\"]\n",
    "            else:\n",
    "                if len(iid) < w_min:\n",
    "                    continue\n",
    "                \n",
    "                missing_length = w_size - 2 - len(lb)\n",
    "                iid = iid + [0]*missing_length\n",
    "                am = am + [0]*missing_length\n",
    "                tti = tti + [0]*missing_length\n",
    "                lb = lb + [\"O\"]*(missing_length+1)\n",
    "                \n",
    "            input_ids_post.append(iid)\n",
    "            attention_mask_post.append(am)\n",
    "            token_type_ids_post.append(tti)\n",
    "            labels_post.append(lb)\n",
    "            \n",
    "    return (input_ids_post,\n",
    "            attention_mask_post,\n",
    "            token_type_ids_post,\n",
    "            labels_post)\n",
    "\n",
    "tags_vals = [\"UNKNOWN\", \"Name\", \"Degree\",\"Skills\",\"College Name\",\"Email Address\",\"Designation\",\"Companies worked at\",\"Empty\",\"Graduation Year\",\"Years of Experience\",\"Location\",\"O\"]\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "\n",
    "def vectorize_df(df):\n",
    "    tokenized_texts = tokenizer(df[\"clean_content\"].tolist())\n",
    "    labels = [align_labels(txt,label) for txt,label in zip(df[\"clean_content\"],df['entities_mapped'])]\n",
    "\n",
    "    # Use spanning window\n",
    "    (tokenized_texts[\"input_ids\"],\n",
    "    tokenized_texts[\"attention_mask\"],\n",
    "    tokenized_texts[\"token_type_ids\"],\n",
    "    labels) = spanning_window(input_ids=tokenized_texts[\"input_ids\"],\n",
    "                                attention_mask=tokenized_texts[\"attention_mask\"],\n",
    "                                token_type_ids=tokenized_texts[\"token_type_ids\"],\n",
    "                                labels=labels,\n",
    "                                w_size=MAX_LEN,\n",
    "                                stride=STRIDE,\n",
    "                                w_min=MIN_LEN)\n",
    "    \n",
    "    label_ids = [list(map(tag2idx.__getitem__,lab)) for lab in labels]\n",
    "\n",
    "    tokenized_texts_pt = {k:torch.tensor(v) for k,v in tokenized_texts.items()}\n",
    "    label_ids_pt = torch.tensor(label_ids,dtype=torch.int64)\n",
    "    \n",
    "    return tokenized_texts_pt, label_ids_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_data,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = vectorize_df(df_train)\n",
    "X_test, y_test = vectorize_df(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2866, 128])\n",
      "torch.Size([393, 128])\n"
     ]
    }
   ],
   "source": [
    "print(X_train[\"input_ids\"].size())\n",
    "print(X_test[\"input_ids\"].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "class ResumeNERLM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 out_classes=len(tags_vals),\n",
    "                 lm=\"dslim/bert-base-NER\",\n",
    "                 device=device):\n",
    "        super(ResumeNERLM,self).__init__()\n",
    "\n",
    "        conf = AutoConfig.from_pretrained(lm)\n",
    "        conf.output_hidden_states = True\n",
    "        model = AutoModelForTokenClassification.from_config(conf).to(device)\n",
    "        \n",
    "        classification_layer = nn.Linear(768,out_classes).to(device)\n",
    "        \n",
    "        self.model = model\n",
    "        self.classification_layer = classification_layer\n",
    "        \n",
    "        self.optimizer = optim.Adam(model.parameters(),lr=5e-5)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self,**x):\n",
    "        x = self.model(**x)\n",
    "        x = x[\"hidden_states\"][-1]\n",
    "        x = [self.classification_layer(x[:,i]) for i in range(1,x.size()[1])]\n",
    "        x = torch.stack(x,dim=1)\n",
    "                        \n",
    "        return x\n",
    "    \n",
    "    def fit(self,X,y,bs=bs,epochs=6):\n",
    "        samples,seq_len = X[\"input_ids\"].size()\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            perm = np.random.permutation(samples)\n",
    "            \n",
    "            trange = tqdm.trange(0,samples,bs)\n",
    "            loss_arr = []\n",
    "            for b_start in trange:\n",
    "                self.optimizer.zero_grad()\n",
    "                b_slice = slice(b_start,b_start+bs)\n",
    "\n",
    "                xi = {k: v[perm[b_slice]].to(self.device) for k,v in X.items()}\n",
    "                yi = y[perm[b_slice]].to(self.device)\n",
    "\n",
    "                x = self(**xi)\n",
    "\n",
    "                losses = torch.mean(torch.stack([self.loss(x[:,i],yi[:,i]) for i in range(seq_len-1)]))\n",
    "                losses = torch.mean(losses)\n",
    "                \n",
    "                loss_arr.append(losses.item())\n",
    "                trange.set_postfix(loss=np.mean(loss_arr))\n",
    "                                \n",
    "                losses.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "    def predict(self,X,bs=32):\n",
    "        samples,seq_len = X[\"input_ids\"].size()\n",
    "        trange = tqdm.trange(0,samples,bs)\n",
    "        \n",
    "        for b_start in trange:\n",
    "            b_slice = slice(b_start,b_start+bs)\n",
    "\n",
    "            xi = {k: v[b_slice].to(self.device) for k,v in X.items()}\n",
    "            x = self(**xi)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "model = ResumeNERLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█████▌                        | 67/359 [00:09<00:40,  7.22it/s, loss=0.807]"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
